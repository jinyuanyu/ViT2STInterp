{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import VideoMAEForPreTraining, VideoMAEImageProcessor\n",
    "\n",
    "# 1. 加载预训练的 VideoMAE 模型和图像处理器\n",
    "model_name = \"MCG-NJU/videomae-base\"\n",
    "model = VideoMAEForPreTraining.from_pretrained(model_name)\n",
    "processor = VideoMAEImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. 准备数据和掩码\n",
    "num_frames = 16\n",
    "channels = 3\n",
    "height = 224\n",
    "width = 224\n",
    "\n",
    "# 创建示例数据和掩码\n",
    "data = np.random.randn(num_frames, channels, height, width).astype(np.float32) #* 16,3,224,224\n",
    "\n",
    "# 强制数据在 [0, 1] 范围内\n",
    "data = np.clip(data, 0, 1)\n",
    "\n",
    "# 确保数据通道顺序为 [frames, height, width, channels]\n",
    "data = data.transpose(0, 2, 3, 1)#* 16,224,224,3\n",
    "\n",
    "# 将数据和掩码转换为 PyTorch 的 Tensor\n",
    "pixel_values = processor(images=list(data), return_tensors=\"pt\", do_rescale=False).pixel_values\n",
    "#* 16,[3,224,224]\n",
    "# 计算掩码的形状和序列长度\n",
    "num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2 #* (224//16)**2=196\n",
    "tubelet_seq_length = num_frames//model.config.tubelet_size#! tubelet_size:The number of tubelets\n",
    "\n",
    "# 随机生成掩码（0表示缺失区域）\n",
    "bool_masked_pos = torch.randint(0, 2, (1,num_patches_per_frame*tubelet_seq_length)).bool()# generate a array which shape is (1,seq_lem) and its range between (0,1)\n",
    "\n",
    "# 3. 使用模型进行推理，得到编码后的特征表示\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values, bool_masked_pos=bool_masked_pos) # Patches are flattened and embedded into vectors.\n",
    "    #! model pre-processing pixel_values [1, 16, 3, 224, 224] to [1,1568,768] ==[1,1568,16*16*3]==[batch_size,tubelet_size,patch_size]\n",
    "    encoded_features = outputs.logits #* batch_size,embedded_seq_length,hidden_size -> 1,variant,1536\n",
    "    #! the embedded_seq_length would change depending on how many patches were masked.\n",
    "#! Spatiotemporal Consistency: The model does not shuffle or lose the order of patches during processing, \n",
    "#! so any reconstruction attempt (e.g., decoding or projecting back to the pixel space) \n",
    "#! should preserve the original spatial and temporal relationships between patches.\n",
    "# 4. 调整输入形状，确保其形状为 [batch_size, seq_length, feature_dim]\n",
    "# encoded_features = encoded_features.view(1, -1, model.config.hidden_size) # model.config.hidden_size=768\n",
    "projection_layer = torch.nn.Linear(outputs.logits.shape[2], 384)# hidden_size decoder typically epected is 384\n",
    "#! Linear Layer allows for learning and adjusting weights to best fit the data.\n",
    "# 5. 使用 decoder 进行解码\n",
    "#! the model's decoding process handles the reconstruction.\n",
    "decoded_frames = model.decoder(projection_layer(outputs.logits), return_token_num=tubelet_seq_length*num_patches_per_frame)\n",
    "#* 1, 712, 1536\n",
    "linear_layer = torch.nn.Linear(1536, 3 * 14 * 14)  # Project to patch size\n",
    "projected_patches = linear_layer(decoded_frames.logits)  # Shape: (1, 748, 768)\n",
    "projected_patches = projected_patches.view(1,-1, 3, 14, 14)  # Shape: (1, 773, 3, 16, 16)\n",
    "num_frames_reconstructed = projected_patches.shape[1] // tubelet_seq_length\n",
    "reconstructed_video = torch.zeros(1, num_frames_reconstructed, 3, 224, 224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits.shape,model.config.hidden_size,decoded_frames.logits.shape,projected_patches.shape,tubelet_seq_length,bool_masked_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "c:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\transformers\\feature_extraction_utils.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1391116380691528\n",
      "Reconstructed patches shape: torch.Size([1, 753, 2304])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import VideoMAEConfig, VideoMAEForPreTraining, VideoMAEImageProcessor\n",
    "\n",
    "# Create a dummy input tensor\n",
    "input_tensor = torch.randn(1, 24, 3, 224, 224)\n",
    "\n",
    "# Initialize the model\n",
    "config = VideoMAEConfig(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    num_channels=3,\n",
    "    num_frames=24,\n",
    "    tubelet_size=3,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.0,\n",
    "    attention_probs_dropout_prob=0.0,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    qkv_bias=True,\n",
    "    use_mean_pooling=True\n",
    ")\n",
    "model = VideoMAEForPreTraining(config)\n",
    "configuration = model.config\n",
    "# Create a mask for missing values\n",
    "num_patches_per_frame = (config.image_size // config.patch_size) ** 2\n",
    "seq_length = (config.num_frames // config.tubelet_size) * num_patches_per_frame#!patch number of a tubelet\n",
    "bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\n",
    "\n",
    "# Prepare the input for the image processor\n",
    "input_np = input_tensor.squeeze(0).permute(0, 2, 3, 1).numpy()\n",
    "input_np = (input_np - input_np.min()) / (input_np.max() - input_np.min())\n",
    "\n",
    "# Initialize the image processor with custom size\n",
    "image_processor = VideoMAEImageProcessor(\n",
    "    do_resize=False, \n",
    "    size={\"height\": 224, \"width\": 224},\n",
    "    do_normalize=True,\n",
    "    image_mean=[0.5, 0.5, 0.5],\n",
    "    image_std=[0.5, 0.5, 0.5]\n",
    ")\n",
    "\n",
    "# Process the input\n",
    "processed_input = image_processor(\n",
    "    images=list(input_np),\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(\n",
    "    pixel_values=processed_input.pixel_values,\n",
    "    bool_masked_pos=bool_masked_pos\n",
    ")\n",
    "\n",
    "# The output contains the loss and reconstructed patches\n",
    "loss = outputs.loss\n",
    "reconstructed_patches = outputs.logits\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Reconstructed patches shape:\", reconstructed_patches.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed video shape: torch.Size([1, 24, 224, 224, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Unflatten the logits to shape (batch_size, masked_seq, tubelet_size, patch_size, patch_size, num_channels)\n",
    "batch_size, num_masked_patches, flattened_patch_dim = reconstructed_patches.shape\n",
    "patch_size = config.patch_size\n",
    "num_channels = config.num_channels\n",
    "tubelet_size = config.tubelet_size\n",
    "\n",
    "# Compute the number of patches per frame and number of frames\n",
    "num_patches_per_frame = (config.image_size // patch_size) ** 2\n",
    "num_frames = config.num_frames\n",
    "\n",
    "#TODO OR Reshape the logits with different position of tubelet_size \n",
    "reconstructed_patches = reconstructed_patches.view(\n",
    "    batch_size, num_masked_patches, tubelet_size, patch_size, patch_size, num_channels\n",
    ")\n",
    "\n",
    "# Create an empty tensor for the output of shape (batch_size, num_frames, num_channels, image_size, image_size)\n",
    "reconstructed_video = torch.zeros(\n",
    "    batch_size, num_frames,  config.image_size, config.image_size,num_channels\n",
    ")\n",
    "\n",
    "# Get the indices of the masked patches\n",
    "masked_indices = torch.nonzero(bool_masked_pos[0]).flatten()\n",
    "\n",
    "# Loop through each masked patch and place it into the correct position in the original video grid\n",
    "for i, idx in enumerate(masked_indices):\n",
    "    frame_idx = idx // num_patches_per_frame  # Find the corresponding frame\n",
    "    patch_idx = idx % num_patches_per_frame  # Find the corresponding patch within the frame\n",
    "    \n",
    "    # Compute the top-left corner of the patch in the frame\n",
    "    y = (patch_idx // (config.image_size // patch_size)) * patch_size\n",
    "    x = (patch_idx % (config.image_size // patch_size)) * patch_size\n",
    "    \n",
    "    for j in range(tubelet_size):\n",
    "    # Place the reconstructed patch back into the original video grid\n",
    "        reconstructed_video[:, frame_idx+j*num_frames//tubelet_size,  y:y+patch_size, x:x+patch_size,:] = reconstructed_patches[:, i,j]\n",
    "\n",
    "# The reconstructed_video now has the same spatial and temporal dimensions as the original input\n",
    "print(\"Reconstructed video shape:\", reconstructed_video.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, VideoMAEForPreTraining\n",
    "import numpy as np\n",
    "import torch\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_frames = 16\n",
    "video = list(np.random.randint(0, 256, (num_frames, 3, 224, 224)))\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "# Move the model to GPU\n",
    "# model.to(device)\n",
    "pixel_values = image_processor(video, return_tensors=\"pt\").pixel_values\n",
    "# Move the inputs to GPU\n",
    "# pixel_values = pixel_values.to(device)\n",
    "num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\n",
    "seq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame#!patch number of a tubelet\n",
    "bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\n",
    "\n",
    "outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\n",
    "loss = outputs.loss\n",
    "print(\"Reconstructed patches shape:\", outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume `input_tensor` is the original input video tensor with shape (batch_size, num_frames, num_channels, height, width)\n",
    "# Squeeze to remove the batch dimension and permute to match the shape of the reconstructed video\n",
    "original_video = input_tensor.squeeze(0).permute(0, 2, 3, 1)  # Shape: (num_frames, height, width, num_channels)\n",
    "\n",
    "# Permute reconstructed_video to match the original video shape\n",
    "reconstructed_video = reconstructed_video.permute(0, 1, 4, 2, 3)  # Shape: (batch_size, num_frames, num_channels, height, width)\n",
    "\n",
    "# Compute the MSE loss\n",
    "loss = F.mse_loss(reconstructed_video, original_video)\n",
    "\n",
    "print(\"Manual Reconstruction Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d20bb8c9b64b06af01ddc5d89425e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eating_spaghetti.mp4:   0%|          | 0.00/1.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\1\\.cache\\huggingface\\hub\\datasets--nielsr--video-demo. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\transformers\\feature_extraction_utils.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1568, 768]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoImageProcessor, VideoMAEModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# video clip consists of 300 frames (10 seconds at 30 FPS)\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
    ")\n",
    "container = av.open(file_path)\n",
    "\n",
    "# sample 16 frames\n",
    "indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "video = read_video_pyav(container, indices)\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "\n",
    "# prepare video for the model\n",
    "inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "# forward pass\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT",
   "language": "python",
   "name": "vit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
