{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import VideoMAEForPreTraining, VideoMAEImageProcessor\n",
    "\n",
    "# 1. 加载预训练的 VideoMAE 模型和图像处理器\n",
    "model_name = \"MCG-NJU/videomae-base\"\n",
    "model = VideoMAEForPreTraining.from_pretrained(model_name)\n",
    "processor = VideoMAEImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. 准备数据和掩码\n",
    "num_frames = 16\n",
    "channels = 3\n",
    "height = 224\n",
    "width = 224\n",
    "\n",
    "# 创建示例数据和掩码\n",
    "data = np.random.randn(num_frames, channels, height, width).astype(np.float32) #* 16,3,224,224\n",
    "\n",
    "# 强制数据在 [0, 1] 范围内\n",
    "data = np.clip(data, 0, 1)\n",
    "\n",
    "# 确保数据通道顺序为 [frames, height, width, channels]\n",
    "data = data.transpose(0, 2, 3, 1)#* 16,224,224,3\n",
    "\n",
    "# 将数据和掩码转换为 PyTorch 的 Tensor\n",
    "pixel_values = processor(images=list(data), return_tensors=\"pt\", do_rescale=False).pixel_values\n",
    "#* 16,[3,224,224]\n",
    "# 计算掩码的形状和序列长度\n",
    "num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2 #* (224//16)**2=196\n",
    "tubelet_seq_length = num_frames//model.config.tubelet_size#! tubelet_size:The number of tubelets\n",
    "\n",
    "# 随机生成掩码（0表示缺失区域）\n",
    "bool_masked_pos = torch.randint(0, 2, (1,num_patches_per_frame*tubelet_seq_length)).bool()# generate a array which shape is (1,seq_lem) and its range between (0,1)\n",
    "\n",
    "# 3. 使用模型进行推理，得到编码后的特征表示\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values, bool_masked_pos=bool_masked_pos) # Patches are flattened and embedded into vectors.\n",
    "    #! model pre-processing pixel_values [1, 16, 3, 224, 224] to [1,1568,768] ==[1,1568,16*16*3]==[batch_size,tubelet_size,patch_size]\n",
    "    encoded_features = outputs.logits #* batch_size,embedded_seq_length,hidden_size -> 1,variant,1536\n",
    "    #! the embedded_seq_length would change depending on how many patches were masked.\n",
    "#! Spatiotemporal Consistency: The model does not shuffle or lose the order of patches during processing, \n",
    "#! so any reconstruction attempt (e.g., decoding or projecting back to the pixel space) \n",
    "#! should preserve the original spatial and temporal relationships between patches.\n",
    "# 4. 调整输入形状，确保其形状为 [batch_size, seq_length, feature_dim]\n",
    "# encoded_features = encoded_features.view(1, -1, model.config.hidden_size) # model.config.hidden_size=768\n",
    "projection_layer = torch.nn.Linear(outputs.logits.shape[2], 384)# hidden_size decoder typically epected is 384\n",
    "#! Linear Layer allows for learning and adjusting weights to best fit the data.\n",
    "# 5. 使用 decoder 进行解码\n",
    "#! the model's decoding process handles the reconstruction.\n",
    "decoded_frames = model.decoder(projection_layer(outputs.logits), return_token_num=tubelet_seq_length*num_patches_per_frame)\n",
    "#* 1, 712, 1536\n",
    "linear_layer = torch.nn.Linear(1536, 3 * 14 * 14)  # Project to patch size\n",
    "projected_patches = linear_layer(decoded_frames.logits)  # Shape: (1, 748, 768)\n",
    "projected_patches = projected_patches.view(1,-1, 3, 14, 14)  # Shape: (1, 773, 3, 16, 16)\n",
    "num_frames_reconstructed = projected_patches.shape[1] // tubelet_seq_length\n",
    "reconstructed_video = torch.zeros(1, num_frames_reconstructed, 3, 224, 224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits.shape,model.config.hidden_size,decoded_frames.logits.shape,projected_patches.shape,tubelet_seq_length,bool_masked_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Make sure that the channel dimension of the pixel values match with the one set in the configuration.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 93\u001b[0m\n\u001b[0;32m     71\u001b[0m bool_masked_pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, (\u001b[38;5;241m1\u001b[39m, seq_length))\u001b[38;5;241m.\u001b[39mbool()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# # Prepare the input for the image processor\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# input_np = input_tensor.squeeze(0).permute(0, 2, 3, 1).numpy()\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# input_np = (input_np - input_np.min()) / (input_np.max() - input_np.min())\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m \n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m     94\u001b[0m     pixel_values\u001b[38;5;241m=\u001b[39minput_tensor,\u001b[38;5;66;03m#processed_input.pixel_values,\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos\n\u001b[0;32m     96\u001b[0m )\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# The output contains the loss and reconstructed patches\u001b[39;00m\n\u001b[0;32m     99\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:850\u001b[0m, in \u001b[0;36mVideoMAEForPreTraining.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;124;03mbool_masked_pos (`torch.BoolTensor` of shape `(batch_size, sequence_length)`):\u001b[39;00m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m    Boolean masked positions. Indicates which patches are masked (1) and which aren't (0). Each video in the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;124;03m>>> loss = outputs.loss\u001b[39;00m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m    848\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 850\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideomae(\n\u001b[0;32m    851\u001b[0m     pixel_values,\n\u001b[0;32m    852\u001b[0m     bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos,\n\u001b[0;32m    853\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    854\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    855\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    856\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    857\u001b[0m )\n\u001b[0;32m    859\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    860\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_to_decoder(\n\u001b[0;32m    861\u001b[0m     sequence_output\n\u001b[0;32m    862\u001b[0m )  \u001b[38;5;66;03m# [batch_size, num_visible_patches, decoder_hidden_size]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:695\u001b[0m, in \u001b[0;36mVideoMAEModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    693\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 695\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, bool_masked_pos)\n\u001b[0;32m    697\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    698\u001b[0m     embedding_output,\n\u001b[0;32m    699\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    702\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    703\u001b[0m )\n\u001b[0;32m    704\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:132\u001b[0m, in \u001b[0;36mVideoMAEEmbeddings.forward\u001b[1;34m(self, pixel_values, bool_masked_pos)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values, bool_masked_pos):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# create patch embeddings\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embeddings(pixel_values)\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# add position embeddings\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embeddings \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings\u001b[38;5;241m.\u001b[39mtype_as(embeddings)\u001b[38;5;241m.\u001b[39mto(embeddings\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:186\u001b[0m, in \u001b[0;36mVideoMAEPatchEmbeddings.forward\u001b[1;34m(self, pixel_values)\u001b[0m\n\u001b[0;32m    184\u001b[0m batch_size, num_frames, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_channels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_channels:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that the channel dimension of the pixel values match with the one set in the configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m height \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m width \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Make sure that the channel dimension of the pixel values match with the one set in the configuration."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import VideoMAEConfig, VideoMAEForPreTraining, VideoMAEImageProcessor\n",
    "\n",
    "# Create a dummy input tensor\n",
    "input_tensor = torch.randn(1, 24, 3, 224, 224)\n",
    "\n",
    "# Initialize the model\n",
    "config = VideoMAEConfig(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    num_channels=3,\n",
    "    num_frames=24,\n",
    "    tubelet_size=3,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.0,\n",
    "    attention_probs_dropout_prob=0.0,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    qkv_bias=True,\n",
    "    use_mean_pooling=True\n",
    ")\n",
    "model = VideoMAEForPreTraining(config)\n",
    "configuration = model.config\n",
    "# Create a mask for missing values\n",
    "num_patches_per_frame = (config.image_size // config.patch_size) ** 2\n",
    "seq_length = (config.num_frames // config.tubelet_size) * num_patches_per_frame#!patch number of a tubelet\n",
    "bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\n",
    "\n",
    "# Prepare the input for the image processor\n",
    "input_np = input_tensor.squeeze(0).permute(0, 2, 3, 1).numpy()\n",
    "input_np = (input_np - input_np.min()) / (input_np.max() - input_np.min())\n",
    "\n",
    "# Initialize the image processor with custom size\n",
    "image_processor = VideoMAEImageProcessor(\n",
    "    do_resize=False, \n",
    "    size={\"height\": 224, \"width\": 224},\n",
    "    do_normalize=True,\n",
    "    image_mean=[0.5, 0.5, 0.5],\n",
    "    image_std=[0.5, 0.5, 0.5]\n",
    ")\n",
    "\n",
    "# Process the input\n",
    "processed_input = image_processor(\n",
    "    images=list(input_np),\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(\n",
    "    pixel_values=processed_input.pixel_values,\n",
    "    bool_masked_pos=bool_masked_pos\n",
    ")\n",
    "\n",
    "# The output contains the loss and reconstructed patches\n",
    "loss = outputs.loss\n",
    "reconstructed_patches = outputs.logits\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Reconstructed patches shape:\", reconstructed_patches.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_26388\\3976486677.py:65: UserWarning: Using a target size (torch.Size([16, 16, 3])) that is different to the input size (torch.Size([1, 16, 16, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  patch_loss = F.mse_loss(reconstructed_patches2[:, masked_patch_idx, j], original_patch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed video shape: torch.Size([1, 24, 224, 224, 3])\n",
      "Manual Reconstruction Loss (masked patches only): 1.1547527313232422\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming `reconstructed_patches` and `input_tensor` are already defined\n",
    "# Unflatten the logits\n",
    "batch_size, num_masked_patches, flattened_patch_dim = reconstructed_patches.shape\n",
    "patch_size = config.patch_size\n",
    "num_channels = config.num_channels\n",
    "tubelet_size = config.tubelet_size\n",
    "\n",
    "# Compute the number of patches per frame and number of frames\n",
    "num_patches_per_frame = (config.image_size // patch_size) ** 2\n",
    "num_frames = config.num_frames\n",
    "\n",
    "# Reshape the logits\n",
    "reconstructed_patches2 = reconstructed_patches.view(\n",
    "    #* 1.1514124870300293-1.1513104438781738-1.1516104936599731-1.1505827903747559\n",
    "    #* -1.151930809020996-1.1523127555847168-1.1507432460784912\n",
    "    # batch_size, num_masked_patches, tubelet_size, patch_size, patch_size, num_channels\n",
    "    # batch_size, num_masked_patches, tubelet_size, num_channels,patch_size, patch_size\n",
    "    # batch_size, num_masked_patches,patch_size, patch_size, tubelet_size, num_channels\n",
    "    batch_size, num_masked_patches,patch_size, patch_size, num_channels,tubelet_size\n",
    "    # batch_size, num_masked_patches,patch_size, patch_size, tubelet_size,num_channels\n",
    "    # batch_size, num_masked_patches,num_channels,patch_size, patch_size,tubelet_size\n",
    "    # batch_size, num_masked_patches,num_channels,tubelet_size,patch_size, patch_size\n",
    ")\n",
    "# reconstructed_patches2=reconstructed_patches2.permute(0,1,2,4,5,3)\n",
    "# reconstructed_patches2=reconstructed_patches2.permute(0,1,4,2,3,5)\n",
    "reconstructed_patches2=reconstructed_patches2.permute(0,1,5,2,3,4)\n",
    "# reconstructed_patches2=reconstructed_patches2.permute(0,1,4,2,3,5)\n",
    "# reconstructed_patches2=reconstructed_patches2.permute(0,1,5,3,4,2)\n",
    "# reconstructed_patches2=reconstructed_patches2.permute(0,1,3,4,5,2)\n",
    "# Create an empty tensor for the output of shape (batch_size, num_frames, num_channels, image_size, image_size)\n",
    "reconstructed_video = torch.zeros(\n",
    "    batch_size, num_frames, config.image_size, config.image_size, num_channels\n",
    ")\n",
    "\n",
    "# Get the indices of the masked patches\n",
    "masked_indices = torch.nonzero(bool_masked_pos[0]).flatten()\n",
    "\n",
    "# Get the indices of the unmasked patches\n",
    "unmasked_indices = torch.nonzero(~bool_masked_pos[0]).flatten()\n",
    "\n",
    "# Initialize a list to store the losses for each masked patch\n",
    "losses = []\n",
    "\n",
    "# Loop through each patch position\n",
    "for frame_idx in range(num_frames):\n",
    "    for patch_idx in range(num_patches_per_frame):\n",
    "        y = (patch_idx // (config.image_size // patch_size)) * patch_size\n",
    "        x = (patch_idx % (config.image_size // patch_size)) * patch_size\n",
    "        \n",
    "        # Determine if the current patch is masked\n",
    "        patch_index_in_masked = frame_idx * num_patches_per_frame + patch_idx\n",
    "        \n",
    "        if patch_index_in_masked in masked_indices:\n",
    "            # If the patch is masked, place the reconstructed patch back into the original video grid\n",
    "            masked_patch_idx = masked_indices.tolist().index(patch_index_in_masked)\n",
    "            for j in range(tubelet_size):\n",
    "                reconstructed_video[:, frame_idx+j*num_frames//tubelet_size, y:y+patch_size, x:x+patch_size, :] = reconstructed_patches2[:, masked_patch_idx, j]\n",
    "\n",
    "                # Compute the MSE loss for this patch\n",
    "                original_patch = input_tensor[:, frame_idx+j*num_frames//tubelet_size, :, y:y+patch_size, x:x+patch_size]\n",
    "                original_patch = original_patch[0].permute(1, 2, 0)\n",
    "                patch_loss = F.mse_loss(reconstructed_patches2[:, masked_patch_idx, j], original_patch)\n",
    "                losses.append(patch_loss)\n",
    "        elif patch_index_in_masked in unmasked_indices:\n",
    "            # If the patch is not masked, copy the original patch to the reconstructed video\n",
    "            for j in range(tubelet_size):\n",
    "                reconstructed_video[:, frame_idx+j*num_frames//tubelet_size, y:y+patch_size, x:x+patch_size, :] = input_tensor[:, frame_idx+j*num_frames//tubelet_size, :, y:y+patch_size, x:x+patch_size].permute(0, 2, 3, 1)\n",
    "\n",
    "# The reconstructed_video now has the same spatial and temporal dimensions as the original input\n",
    "print(\"Reconstructed video shape:\", reconstructed_video.shape)\n",
    "\n",
    "# Compute the mean loss across all masked patches\n",
    "mean_loss = torch.mean(torch.stack(losses))\n",
    "print(\"Manual Reconstruction Loss (masked patches only):\", mean_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 24, 224, 224, 3]), tensor(0.5626, grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.permute(0,1,3,4,2).shape,F.mse_loss(reconstructed_video,input_tensor.permute(0,1,3,4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, VideoMAEForPreTraining\n",
    "import numpy as np\n",
    "import torch\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_frames = 16\n",
    "video = list(np.random.randint(0, 256, (num_frames, 3, 224, 224)))\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "# Move the model to GPU\n",
    "# model.to(device)\n",
    "pixel_values = image_processor(video, return_tensors=\"pt\").pixel_values\n",
    "# Move the inputs to GPU\n",
    "# pixel_values = pixel_values.to(device)\n",
    "num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\n",
    "seq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame#!patch number of a tubelet\n",
    "bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\n",
    "\n",
    "outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\n",
    "loss = outputs.loss\n",
    "print(\"Reconstructed patches shape:\", outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d20bb8c9b64b06af01ddc5d89425e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eating_spaghetti.mp4:   0%|          | 0.00/1.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\1\\.cache\\huggingface\\hub\\datasets--nielsr--video-demo. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\1\\.conda\\envs\\ViT\\Lib\\site-packages\\transformers\\feature_extraction_utils.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1568, 768]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoImageProcessor, VideoMAEModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# video clip consists of 300 frames (10 seconds at 30 FPS)\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
    ")\n",
    "container = av.open(file_path)\n",
    "\n",
    "# sample 16 frames\n",
    "indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "video = read_video_pyav(container, indices)\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "\n",
    "# prepare video for the model\n",
    "inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "# forward pass\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT",
   "language": "python",
   "name": "vit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
