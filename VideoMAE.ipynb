{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import VideoMAEForPreTraining, VideoMAEImageProcessor\n",
    "\n",
    "# 1. 加载预训练的 VideoMAE 模型和图像处理器\n",
    "model_name = \"MCG-NJU/videomae-base\"\n",
    "model = VideoMAEForPreTraining.from_pretrained(model_name)\n",
    "processor = VideoMAEImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. 准备数据和掩码\n",
    "num_frames = 16\n",
    "channels = 3\n",
    "height = 224\n",
    "width = 224\n",
    "\n",
    "# 创建示例数据和掩码\n",
    "data = np.random.randn(num_frames, channels, height, width).astype(np.float32) #* 16,3,224,224\n",
    "\n",
    "# 强制数据在 [0, 1] 范围内\n",
    "data = np.clip(data, 0, 1)\n",
    "\n",
    "# 确保数据通道顺序为 [frames, height, width, channels]\n",
    "data = data.transpose(0, 2, 3, 1)#* 16,224,224,3\n",
    "\n",
    "# 将数据和掩码转换为 PyTorch 的 Tensor\n",
    "pixel_values = processor(images=list(data), return_tensors=\"pt\", do_rescale=False).pixel_values\n",
    "#* 16,[3,224,224]\n",
    "# 计算掩码的形状和序列长度\n",
    "num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2 #* (224//16)**2=196\n",
    "tubelet_seq_length = num_frames//model.config.tubelet_size#! tubelet_size:The number of tubelets\n",
    "\n",
    "# 随机生成掩码（0表示缺失区域）\n",
    "bool_masked_pos = torch.randint(0, 2, (1,num_patches_per_frame*tubelet_seq_length)).bool()# generate a array which shape is (1,seq_lem) and its range between (0,1)\n",
    "\n",
    "# 3. 使用模型进行推理，得到编码后的特征表示\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values, bool_masked_pos=bool_masked_pos) # Patches are flattened and embedded into vectors.\n",
    "    #! model pre-processing pixel_values [1, 16, 3, 224, 224] to [1,1568,768] ==[1,1568,16*16*3]==[batch_size,tubelet_size,patch_size]\n",
    "    encoded_features = outputs.logits #* batch_size,embedded_seq_length,hidden_size -> 1,variant,1536\n",
    "    #! the embedded_seq_length would change depending on how many patches were masked.\n",
    "#! Spatiotemporal Consistency: The model does not shuffle or lose the order of patches during processing, \n",
    "#! so any reconstruction attempt (e.g., decoding or projecting back to the pixel space) \n",
    "#! should preserve the original spatial and temporal relationships between patches.\n",
    "# 4. 调整输入形状，确保其形状为 [batch_size, seq_length, feature_dim]\n",
    "# encoded_features = encoded_features.view(1, -1, model.config.hidden_size) # model.config.hidden_size=768\n",
    "projection_layer = torch.nn.Linear(outputs.logits.shape[2], 384)# hidden_size decoder typically epected is 384\n",
    "#! Linear Layer allows for learning and adjusting weights to best fit the data.\n",
    "# 5. 使用 decoder 进行解码\n",
    "#! the model's decoding process handles the reconstruction.\n",
    "decoded_frames = model.decoder(projection_layer(outputs.logits), return_token_num=tubelet_seq_length*num_patches_per_frame)\n",
    "#* 1, 712, 1536\n",
    "linear_layer = torch.nn.Linear(1536, 3 * 14 * 14)  # Project to patch size\n",
    "projected_patches = linear_layer(decoded_frames.logits)  # Shape: (1, 748, 768)\n",
    "projected_patches = projected_patches.view(1,-1, 3, 14, 14)  # Shape: (1, 773, 3, 16, 16)\n",
    "num_frames_reconstructed = projected_patches.shape[1] // tubelet_seq_length\n",
    "reconstructed_video = torch.zeros(1, num_frames_reconstructed, 3, 224, 224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits.shape,model.config.hidden_size,decoded_frames.logits.shape,projected_patches.shape,tubelet_seq_length,bool_masked_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import VideoMAEConfig, VideoMAEForPreTraining, VideoMAEImageProcessor\n",
    "\n",
    "# Create a dummy input tensor\n",
    "input_tensor = torch.randn(1, 24, 3, 224, 224)\n",
    "\n",
    "# Initialize the model\n",
    "config = VideoMAEConfig(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    num_channels=3,\n",
    "    num_frames=24,\n",
    "    tubelet_size=3,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.0,\n",
    "    attention_probs_dropout_prob=0.0,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    qkv_bias=True,\n",
    "    use_mean_pooling=True\n",
    ")\n",
    "model = VideoMAEForPreTraining(config)\n",
    "configuration = model.config\n",
    "# Create a mask for missing values\n",
    "num_patches_per_frame = (config.image_size // config.patch_size) ** 2\n",
    "seq_length = (config.num_frames // config.tubelet_size) * num_patches_per_frame#!patch number of a tubelet\n",
    "bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\n",
    "\n",
    "# Prepare the input for the image processor\n",
    "input_np = input_tensor.squeeze(0).permute(0, 2, 3, 1).numpy()\n",
    "input_np = (input_np - input_np.min()) / (input_np.max() - input_np.min())\n",
    "\n",
    "# Initialize the image processor with custom size\n",
    "image_processor = VideoMAEImageProcessor(\n",
    "    do_resize=False, \n",
    "    size={\"height\": 224, \"width\": 224},\n",
    "    do_normalize=True,\n",
    "    image_mean=[0.5, 0.5, 0.5],\n",
    "    image_std=[0.5, 0.5, 0.5]\n",
    ")\n",
    "\n",
    "# Process the input\n",
    "processed_input = image_processor(\n",
    "    images=list(input_np),\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(\n",
    "    pixel_values=processed_input.pixel_values,\n",
    "    bool_masked_pos=bool_masked_pos\n",
    ")\n",
    "\n",
    "# The output contains the loss and reconstructed patches\n",
    "loss = outputs.loss\n",
    "reconstructed_patches = outputs.logits\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Reconstructed patches shape:\", reconstructed_patches.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import av\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, VideoMAEForPreTraining, VideoMAEConfig\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize the VideoMAE model for pre-training\n",
    "config = VideoMAEConfig(\n",
    "    image_size=112,\n",
    "    patch_size=7,\n",
    "    num_channels=3,\n",
    "    num_frames=16,\n",
    "    tubelet_size=2,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.0,\n",
    "    attention_probs_dropout_prob=0.0,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    qkv_bias=True,\n",
    "    use_mean_pooling=True\n",
    ")\n",
    "# Function to read and sample video frames (provided)\n",
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "# Download and extract frames from the video\n",
    "file_path = hf_hub_download(repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\")\n",
    "container = av.open(file_path)\n",
    "\n",
    "# Sample 24(num_frames) frames (matching the original input tensor shape)\n",
    "indices = sample_frame_indices(clip_len=config.num_frames, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "video_frames = read_video_pyav(container, indices)\n",
    "\n",
    "def read_video(frames, frame_size):\n",
    "    # Define the transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(frame_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Convert each frame (which is a NumPy array) to a PIL Image and apply the transformations\n",
    "    processed_frames = torch.stack([transform(Image.fromarray(frame)) for frame in frames])\n",
    "    \n",
    "    return processed_frames\n",
    "\n",
    "processed_input = read_video(video_frames, (config.image_size, config.image_size))\n",
    "input_tensor = processed_input.unsqueeze(0)\n",
    "input_tensor[0, 0, :, :, :] = 1.0#* set first frame all white\n",
    "# # Initialize the image processor from the pretrained model\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "# # Process the video frames into a tensor\n",
    "# processed_input = image_processor(images=list(video_frames), return_tensors=\"pt\")\n",
    "# input_tensor = processed_input.pixel_values\n",
    "\n",
    "\n",
    "model = VideoMAEForPreTraining(config)\n",
    "\n",
    "# Create a mask for missing values\n",
    "num_patches_per_frame = (config.image_size // config.patch_size) ** 2\n",
    "#! seq_length:patch number of a tubelet\n",
    "seq_length = (config.num_frames // config.tubelet_size) * num_patches_per_frame\n",
    "bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\n",
    "\n",
    "# Forward pass through the model using the preprocessed video frames\n",
    "outputs = model(pixel_values=input_tensor, bool_masked_pos=bool_masked_pos)\n",
    "\n",
    "# The output contains the loss and reconstructed patches\n",
    "loss = outputs.loss\n",
    "reconstructed_patches = outputs.logits\n",
    "\n",
    "# Display the loss and shape of the reconstructed patches\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Reconstructed patches shape:\", reconstructed_patches.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "patch_size=(config.patch_size,config.patch_size)\n",
    "orig_frames = input_tensor.permute(0,1,3,4,2)\n",
    "img_squeeze = rearrange(\n",
    "    orig_frames.numpy(), \n",
    "    'b (t p0) (h p1) (w p2) c -> b (t h w) (p0 p1 p2) c', \n",
    "    p0=config.tubelet_size, p1=patch_size[0], p2=patch_size[1]\n",
    ")\n",
    "img_mean = np.mean(img_squeeze, axis=-2, keepdims=True)\n",
    "img_variance = np.var(img_squeeze, axis=-2, ddof=1, keepdims=True)\n",
    "img_norm = (img_squeeze - img_mean) / (np.sqrt(img_variance) + 1e-6)\n",
    "img_patch = rearrange(img_norm, 'b n p c -> b n (p c)')\n",
    "img_patch[bool_masked_pos] = outputs.logits.detach().numpy()\n",
    "rec_img = rearrange(img_patch, 'b n (p c) -> b n p c', c=config.num_channels)\n",
    "# Notice: To visualize the reconstruction video, \n",
    "# we add the predict and the original mean and var of each patch.\n",
    "img_mean = np.mean(img_squeeze, axis=-2, keepdims=True)\n",
    "img_std = np.sqrt(np.var(img_squeeze, axis=-2, ddof=1, keepdims=True) + 1e-6) \n",
    "rec_img = rec_img * img_std + img_mean\n",
    "rec_img = rearrange(\n",
    "    rec_img, \n",
    "    'b (t h w) (p0 p1 p2) c -> b (t p0) (h p1) (w p2) c', \n",
    "    p0=config.tubelet_size, p1=patch_size[0], p2=patch_size[1], h=config.image_size//config.patch_size, w=config.image_size//config.patch_size\n",
    ")\n",
    "print(rec_img.shape)\n",
    "reconstructed_video=rec_img\n",
    "\n",
    "mask=np.ones_like (img_patch)\n",
    "mask[bool_masked_pos]=0\n",
    "mask=rearrange(mask,'b n (p c)->b n p c',c=config.num_channels)\n",
    "mask=rearrange(\n",
    "    mask,\n",
    "    'b (t h w) (p0 pl p2) c->b (t p0) (h pl) (w p2) c',\n",
    "    p0=config.tubelet_size,pl=patch_size[0],p2=patch_size[1],h=config.image_size//config.patch_size, w=config.image_size//config.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "IMAGENET_STD=np.array([0.225,0.225,0.225])\n",
    "IMAGENET_MEAN=np.array([0.45,0.45,0.45])\n",
    "def show_sequence(seq,sample=8,title=''):\n",
    "#clip is [1,H,属37\n",
    "    assert (seq.shape[-1]==3)\n",
    "    fig =plt.figure(figsize=(20,2.5))\n",
    "    fig.suptitle(title,fontsize=16)\n",
    "    grid=ImageGrid(fig,111,nrows_ncols=(1,sample),axes_pad=0.1)\n",
    "    for ax,img in zip(grid,seq):\n",
    "        img=img*IMAGENET_STD+IMAGENET_MEAN\n",
    "        ax.imshow((img*255).clip(0,255).astype ('uint8'))\n",
    "        ax.set_axis_off()\n",
    "    plt.show()\n",
    "    return\n",
    "show_sequence(orig_frames.detach().numpy()[0][::-1],sample=6)\n",
    "show_sequence(mask[0] * orig_frames.numpy()[0], sample=6)\n",
    "show_sequence(rec_img[0][::-1],sample=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming `reconstructed_patches` and `input_tensor` are already defined\n",
    "# Unflatten the logits\n",
    "batch_size, num_masked_patches, flattened_patch_dim = reconstructed_patches.shape\n",
    "patch_size = config.patch_size\n",
    "num_channels = config.num_channels\n",
    "tubelet_size = config.tubelet_size\n",
    "\n",
    "# Compute the number of patches per frame and number of frames\n",
    "num_patches_per_frame = (config.image_size // patch_size) ** 2\n",
    "num_frames = config.num_frames\n",
    "\n",
    "# Reshape the logits\n",
    "reconstructed_patches2 = reconstructed_patches.view(\n",
    "    #* 1.1514124870300293-1.1513104438781738-1.1516104936599731-1.1505827903747559\n",
    "    #* -1.151930809020996-1.1523127555847168-1.1507432460784912\n",
    "    # batch_size, num_masked_patches, tubelet_size, patch_size, patch_size, num_channels\n",
    "    # batch_size, num_masked_patches, tubelet_size, num_channels,patch_size, patch_size\n",
    "    # batch_size, num_masked_patches,patch_size, patch_size, tubelet_size, num_channels\n",
    "    # batch_size, num_masked_patches,patch_size, patch_size, num_channels,tubelet_size\n",
    "    # batch_size, num_masked_patches,patch_size, patch_size, tubelet_size,num_channels\n",
    "    # batch_size, num_masked_patches,num_channels,patch_size, patch_size,tubelet_size\n",
    "    batch_size, num_masked_patches,num_channels,tubelet_size,patch_size, patch_size\n",
    ")\n",
    "# reconstructed_patches2=reconstructed_patches2.permute(0,1,2,4,5,3)\n",
    "# reconstructed_patches2=reconstructed_patches2.permute(0,1,4,2,3,5)\n",
    "# reconstructed_patches2=reconstructed_patches2.permute(0,1,5,2,3,4)\n",
    "# reconstructed_patches2=reconstructed_patches2.permute(0,1,4,2,3,5)\n",
    "# reconstructed_patches2=reconstructed_patches2.permute(0,1,5,3,4,2)\n",
    "reconstructed_patches2=reconstructed_patches2.permute(0,1,3,4,5,2)\n",
    "# Create an empty tensor for the output of shape (batch_size, num_frames, num_channels, image_size, image_size)\n",
    "reconstructed_video = torch.zeros(\n",
    "    batch_size, num_frames, config.image_size, config.image_size, num_channels\n",
    ")\n",
    "\n",
    "# Get the indices of the masked patches\n",
    "masked_indices = torch.nonzero(bool_masked_pos[0]).flatten()\n",
    "\n",
    "# Get the indices of the unmasked patches\n",
    "unmasked_indices = torch.nonzero(~bool_masked_pos[0]).flatten()\n",
    "\n",
    "# Initialize a list to store the losses for each masked patch\n",
    "losses = []\n",
    "\n",
    "# Loop through each patch position\n",
    "for frame_idx in range(num_frames):\n",
    "    for patch_idx in range(num_patches_per_frame):\n",
    "        y = (patch_idx // (config.image_size // patch_size)) * patch_size\n",
    "        x = (patch_idx % (config.image_size // patch_size)) * patch_size\n",
    "        \n",
    "        # Determine if the current patch is masked\n",
    "        patch_index_in_masked = frame_idx * num_patches_per_frame + patch_idx\n",
    "        \n",
    "        if patch_index_in_masked in masked_indices:\n",
    "            # If the patch is masked, place the reconstructed patch back into the original video grid\n",
    "            masked_patch_idx = masked_indices.tolist().index(patch_index_in_masked)\n",
    "            for j in range(tubelet_size):\n",
    "                reconstructed_video[:, frame_idx+j*num_frames//tubelet_size, y:y+patch_size, x:x+patch_size, :] = reconstructed_patches2[:, masked_patch_idx, j]\n",
    "\n",
    "                # Compute the MSE loss for this patch\n",
    "                original_patch = input_tensor[:, frame_idx+j*num_frames//tubelet_size, :, y:y+patch_size, x:x+patch_size]\n",
    "                original_patch = original_patch[0].permute(1, 2, 0)\n",
    "                patch_loss = F.mse_loss(reconstructed_patches2[:, masked_patch_idx, j], original_patch)\n",
    "                losses.append(patch_loss)\n",
    "        elif patch_index_in_masked in unmasked_indices:\n",
    "            # If the patch is not masked, copy the original patch to the reconstructed video\n",
    "            for j in range(tubelet_size):\n",
    "                reconstructed_video[:, frame_idx+j*num_frames//tubelet_size, y:y+patch_size, x:x+patch_size, :] = input_tensor[:, frame_idx+j*num_frames//tubelet_size, :, y:y+patch_size, x:x+patch_size].permute(0, 2, 3, 1)\n",
    "\n",
    "# The reconstructed_video now has the same spatial and temporal dimensions as the original input\n",
    "print(\"Reconstructed video shape:\", reconstructed_video.shape)\n",
    "\n",
    "# Compute the mean loss across all masked patches\n",
    "mean_loss = torch.mean(torch.stack(losses))\n",
    "print(\"Manual Reconstruction Loss (masked patches only):\", mean_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "# Assuming `reconstructed_video` is already defined and has shape (batch_size, num_frames, height, width, num_channels)\n",
    "\n",
    "# Convert the tensor to a numpy array, detach first to avoid RuntimeError\n",
    "reconstructed_video_np = reconstructed_video#.detach().cpu().numpy()\n",
    "\n",
    "# Clip values to the valid range [0, 1] and scale to [0, 255]\n",
    "reconstructed_video_np = np.clip(reconstructed_video_np, 0, 1) * 255\n",
    "reconstructed_video_np = reconstructed_video_np.astype(np.uint8)\n",
    "\n",
    "# Transpose the array to (num_frames, height, width, num_channels)\n",
    "reconstructed_video_np = reconstructed_video_np[0].transpose(0, 1, 2, 3)\n",
    "\n",
    "# Initialize a PyAV container for writing the video\n",
    "output_filename = \"output.mp4\"\n",
    "output_container = av.open(output_filename, mode='w')\n",
    "stream = output_container.add_stream('mpeg4', rate=30)  # You can change the codec and frame rate as needed\n",
    "stream.width = config.image_size\n",
    "stream.height = config.image_size\n",
    "stream.pix_fmt = 'yuv420p'\n",
    "\n",
    "# Write each frame to the video\n",
    "for frame in reconstructed_video_np:\n",
    "    frame_rgb = av.VideoFrame.from_ndarray(frame, format='rgb24')\n",
    "    packet = stream.encode(frame_rgb)\n",
    "    output_container.mux(packet)\n",
    "\n",
    "# Finalize the video file\n",
    "output_container.mux(stream.encode())\n",
    "output_container.close()\n",
    "\n",
    "print(f\"Video saved as {output_filename}\")\n",
    "# Assuming `input_tensor` is already defined and has shape (batch_size, num_frames, num_channels, height, width)\n",
    "\n",
    "# Convert the tensor to a numpy array, detach first to avoid RuntimeError\n",
    "input_tensor_np = input_tensor.detach().cpu().numpy()\n",
    "\n",
    "# Clip values to the valid range [0, 1] and scale to [0, 255]\n",
    "input_tensor_np = np.clip(input_tensor_np, 0, 1) * 255\n",
    "input_tensor_np = input_tensor_np.astype(np.uint8)\n",
    "\n",
    "# Transpose the array to (num_frames, height, width, num_channels)\n",
    "input_tensor_np = input_tensor_np[0].transpose(0, 2, 3, 1)\n",
    "\n",
    "# Initialize a PyAV container for writing the video\n",
    "input_filename = \"input.mp4\"\n",
    "input_container = av.open(input_filename, mode='w')\n",
    "stream = input_container.add_stream('mpeg4', rate=30)  # You can change the codec and frame rate as needed\n",
    "stream.width = config.image_size\n",
    "stream.height = config.image_size\n",
    "stream.pix_fmt = 'yuv420p'\n",
    "\n",
    "# Write each frame to the video\n",
    "for frame in input_tensor_np:\n",
    "    frame_rgb = av.VideoFrame.from_ndarray(frame, format='rgb24')\n",
    "    packet = stream.encode(frame_rgb)\n",
    "    input_container.mux(packet)\n",
    "\n",
    "# Finalize the video file\n",
    "input_container.mux(stream.encode())\n",
    "input_container.close()\n",
    "\n",
    "print(f\"Video saved as {input_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_masked_pos[:,13],bool_masked_pos[:,14]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT",
   "language": "python",
   "name": "vit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
