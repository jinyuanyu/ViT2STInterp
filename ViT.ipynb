{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1f2e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\V'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\V'\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_28728\\671440661.py:1: SyntaxWarning: invalid escape sequence '\\V'\n",
      "  '''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 9, 49, 46]) torch.Size([1, 4, 9, 49, 46]) torch.Size([1, 4, 9, 49, 46])\n",
      "torch.Size([1, 4, 9, 49, 46]) torch.Size([1, 4, 9, 49, 46]) torch.Size([1, 4, 9, 49, 46])\n",
      "torch.Size([1, 4, 9, 49, 46]) torch.Size([1, 4, 9, 49, 46]) torch.Size([1, 4, 9, 49, 46])\n",
      "torch.Size([1, 4, 9, 49, 46]) torch.Size([1, 4, 9, 49, 46]) torch.Size([1, 4, 9, 49, 46])\n",
      "torch.Size([1, 4, 9, 49, 46]) torch.Size([1, 4, 9, 49, 46]) torch.Size([1, 4, 9, 49, 46])\n",
      "torch.Size([1, 4, 9, 49, 46]) torch.Size([1, 4, 9, 49, 46]) torch.Size([1, 4, 9, 49, 46])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Author: YJY 522066928@qq.com\n",
    "Date: 2024-08-19 10:59:20\n",
    "LastEditors: YJY 522066928@qq.com\n",
    "LastEditTime: 2024-08-21 15:04:11\n",
    "FilePath: \\ViT2STInterp\\ViT.ipynb\n",
    "Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import netCDF4 as nc\n",
    "from torchvision import transforms\n",
    "\n",
    "scale_factor = 1.3 #数据值域缩放比\n",
    "\n",
    "def load_data_from_directory(directory, scale_factor=1.0):\n",
    "    file_paths = glob.glob(f'{directory}/*.nc')\n",
    "    all_data = []\n",
    "    all_masks = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        dataset = nc.Dataset(file_path, 'r')\n",
    "        file_name = os.path.basename(file_path)\n",
    "        prefix = file_name.split('.')[0]\n",
    "        data_var = dataset.variables[prefix][:]  # Adjust if needed\n",
    "        data_var = np.expand_dims(data_var, axis=1)\n",
    "        mask_var = dataset.variables['mask'][:]\n",
    "        mask_var = np.expand_dims(mask_var, axis=0)\n",
    "        missing_value = dataset.variables[prefix]._FillValue\n",
    "\n",
    "        # Handle missing values\n",
    "        mask = (mask_var == 1).astype(np.float32)\n",
    "        data_var[data_var == missing_value] = 0\n",
    "        data_var = np.nan_to_num(data_var, nan=0.0)\n",
    "\n",
    "        # Apply scaling factor to the data\n",
    "        data_var *= scale_factor\n",
    "        all_data.append(data_var)\n",
    "        all_masks.append(mask)\n",
    "\n",
    "        dataset.close()\n",
    "\n",
    "    # Combine all bands with a new channel dimension\n",
    "\n",
    "    all_data = np.concatenate(all_data, axis=1)  # Merge all bands into channel dimension\n",
    "    all_masks = np.concatenate(all_masks, axis=0)\n",
    "    # Reshape data to fit [batch, time, channels, height, width]\n",
    "    # all_data = np.expand_dims(all_data, axis=0) # [samples, time, channels, height, width]\n",
    "    # all_masks = np.expand_dims(all_masks, axis=0) # [samples, channels, height, width]\n",
    "\n",
    "    return all_data, all_masks\n",
    "\n",
    "class OceanDataset(Dataset):\n",
    "    len_frame=4\n",
    "    def __init__(self, data, mask, transform=None, mask_transform=None, use_random_mask=False, mask_ratio=0.0):\n",
    "        self.data = data # [time, channels, height, width]\n",
    "        self.mask = mask # [channels, height, width]\n",
    "        self.use_random_mask = use_random_mask\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __len__(self,len_frame=len_frame):\n",
    "        return int(self.data.shape[0]/len_frame)  # Number of sample\n",
    "\n",
    "    def __getitem__(self, idx,len_frame=len_frame): #? The size and range of idx depend on the __len__\n",
    "        data_sample = self.data[idx*len_frame:(idx+1)*len_frame, :, :]  # Take the first 4 time frames [channels, time, height, width]\n",
    "        mask_sample = self.mask[0, :, :]  # Corresponding mask frames [batch, channels, height, width]\n",
    "\n",
    "        # Convert to torch tensor\n",
    "        data_sample = torch.tensor(data_sample, dtype=torch.float32)\n",
    "        mask_sample = torch.tensor(mask_sample, dtype=torch.float32)\n",
    "\n",
    "        # Generate mask based on data_sample\n",
    "        data_based_mask = (data_sample == 0).float()\n",
    "        if self.use_random_mask:\n",
    "            random_mask = torch.rand_like(mask_sample) < self.mask_ratio\n",
    "            mask_sample = torch.max(mask_sample, random_mask.float())\n",
    "        mask_sample = torch.max(mask_sample, data_based_mask)\n",
    "        meta_data = (data_sample != 0).float()\n",
    "        return data_sample, mask_sample, meta_data\n",
    "\n",
    "# Load the meta_data\n",
    "data_var, mask = load_data_from_directory('G:/YuJinYuan-Experiment/MODIS', scale_factor)\n",
    "\n",
    "# Create dataset and Dataloader\n",
    "dataset = OceanDataset(data_var, mask, use_random_mask=True, mask_ratio=0.1)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False) #? batch_size=1:it does automatically add an extra dimension \n",
    "\n",
    "# Example: Get one sample and print the shape (batch_size=1)\n",
    "for data_sample, mask_sample, meta_data in dataloader:\n",
    "    print(data_sample.shape,mask_sample.shape,meta_data.shape)  # Should output torch.Size([1, 4, 10, 49, 46])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371526d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNTransformerForInterpolation(nn.Module):\n",
    "    def __init__(self, channels, dim, depth, heads, mlp_dim, patch_size=8, stride=4, cnn_channels=[64, 128]):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.num_heads = heads\n",
    "        \n",
    "        # CNN layers\n",
    "        self.cnn_layers = nn.ModuleList([\n",
    "            nn.Conv3d(self.channels, cnn_channels[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(cnn_channels[0], cnn_channels[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, batch_first=True),\n",
    "            num_layers=depth\n",
    "        )\n",
    "        \n",
    "        self.to_patch_embedding = nn.Linear(patch_size * patch_size * cnn_channels[-1], dim)\n",
    "        self.to_pixel_values = nn.Linear(dim, patch_size * patch_size * self.channels)\n",
    "        \n",
    "        # Final CNN layer to refine output\n",
    "        self.final_cnn = nn.Conv3d(self.channels, self.channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def create_patches(self, img):\n",
    "        B, T, C, H, W = img.shape\n",
    "        patches = img.unfold(3, self.patch_size, self.stride).unfold(4, self.patch_size, self.stride)\n",
    "        patches = patches.contiguous().view(B, T, C, -1, self.patch_size, self.patch_size)\n",
    "        patches = patches.permute(0, 3, 1, 2, 4, 5).reshape(-1, T, C * self.patch_size * self.patch_size)\n",
    "        return patches\n",
    "    \n",
    "    def reconstruct_image(self, patches, H, W):\n",
    "        B, T, C, num_patches, patch_h, patch_w = patches.shape\n",
    "        img = torch.zeros(B, T, C, H, W).to(patches.device)\n",
    "        count = torch.zeros_like(img)\n",
    "        \n",
    "        idx = 0\n",
    "        for i in range(0, H - patch_h + 1, self.stride):\n",
    "            for j in range(0, W - patch_w + 1, self.stride):\n",
    "                img[:, :, :, i:i+patch_h, j:j+patch_w] += patches[:, :, :, idx]\n",
    "                count[:, :, :, i:i+patch_h, j:j+patch_w] += 1\n",
    "                idx += 1\n",
    "        \n",
    "        img /= count\n",
    "        return img\n",
    "    \n",
    "    def forward(self, img):\n",
    "        B, T, C, H, W = img.shape\n",
    "        \n",
    "        # Apply CNN layers\n",
    "        x = img.permute(0, 2, 1, 3, 4)  # [B, C, T, H, W]\n",
    "        for cnn_layer in self.cnn_layers:\n",
    "            x = cnn_layer(x)\n",
    "        \n",
    "        # Create patches\n",
    "        x = x.permute(0, 2, 1, 3, 4)  # [B, T, C, H, W]\n",
    "        patches = self.create_patches(x)\n",
    "        \n",
    "        # Apply Transformer\n",
    "        patches = self.to_patch_embedding(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        \n",
    "        # Recover pixel values\n",
    "        recovered_patches = self.to_pixel_values(patches)\n",
    "        recovered_patches = recovered_patches.view(B, -1, T, C, self.patch_size, self.patch_size)\n",
    "        recovered_patches = recovered_patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        \n",
    "        # Reconstruct image\n",
    "        output = self.reconstruct_image(recovered_patches, H, W)\n",
    "        \n",
    "        # Final refinement\n",
    "        output = output.permute(0, 2, 1, 3, 4)  # [B, C, T, H, W]\n",
    "        output = self.final_cnn(output)\n",
    "        output = output.permute(0, 2, 1, 3, 4)  # [B, T, C, H, W]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "model = CNNTransformerForInterpolation(\n",
    "    channels=9,\n",
    "    dim=256,\n",
    "    depth=12,\n",
    "    heads=16,\n",
    "    mlp_dim=1024,\n",
    "    patch_size=8,\n",
    "    stride=4,\n",
    "    cnn_channels=[64, 128]\n",
    ")\n",
    "\n",
    "# Generate a dummy 4-frame, 10-channel 49x46 image data\n",
    "# input_image = torch.randn(1, 4, 10, 49, 46)  # [batch, frames, channels, height, width]\n",
    "\n",
    "# Perform interpolation\n",
    "output_image = model(data_sample)\n",
    "\n",
    "print(output_image.shape)  # Should be [1, 4, 10, 49, 46]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a32e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1\n",
    "num_training_cycles = 2  # Number of times to retrain with new random masks\n",
    "initial_mask_ratio = 0.2  # Starting mask ratio\n",
    "mask_ratio_increment = 0.1  # Increment of mask ratio per cycle\n",
    "weight_factor = 1.0  # Adjust this factor to match the scaling applied to the data\n",
    "for cycle in range(num_training_cycles):\n",
    "    current_mask_ratio = initial_mask_ratio + cycle * mask_ratio_increment\n",
    "    print(f'Training Cycle {cycle+1}/{num_training_cycles}, Mask Ratio: {current_mask_ratio:.2f}')\n",
    "    \n",
    "    # 生成数据集和数据加载器\n",
    "    train_dataset=dataset\n",
    "    train_loader=dataloader\n",
    "    # train_dataset = OceanDataset(train_data, train_mask,  use_random_mask=True, mask_ratio=current_mask_ratio)\n",
    "    # val_dataset = OceanDataset(val_data, val_mask, use_random_mask=False)\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        for data, mask, meta_data in train_loader:\n",
    "            data = data.to(device)\n",
    "            mask = mask.to(device)\n",
    "            meta_data = meta_data.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(data)\n",
    "            output_image = outputs\n",
    "            # 只在 meta_data 区域计算损失\n",
    "            loss_per_pixel = criterion(outputs, data)\n",
    "            print(loss_per_pixel,outputs.shape,data.shape)\n",
    "            loss = (loss_per_pixel * meta_data).sum() / meta_data.sum()\n",
    "            \n",
    "            # 乘以权重因子\n",
    "            loss *= weight_factor\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        # 计算平均损失\n",
    "        avg_loss = total_loss / batch_count\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "# 保存模型权重\n",
    "# torch.save(model.state_dict(), 'ViTForInterpolation.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c63f0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4431\n",
      "Epoch [2/10], Loss: 0.1399\n",
      "Epoch [3/10], Loss: 0.1054\n",
      "Epoch [4/10], Loss: 0.0880\n",
      "Epoch [5/10], Loss: 0.0881\n",
      "Epoch [6/10], Loss: 0.0884\n",
      "Epoch [7/10], Loss: 0.0866\n",
      "Epoch [8/10], Loss: 0.0841\n",
      "Epoch [9/10], Loss: 0.0837\n",
      "Epoch [10/10], Loss: 0.0840\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Assuming your data is in the form of a tensor of shape (batch_size, num_frames, channels, height, width)\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_data, transform=None):\n",
    "        self.video_data = video_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video = self.video_data[idx]\n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "        return video\n",
    "\n",
    "# Example preprocessing and model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    # transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Example video data (random tensor for illustration)\n",
    "video_data = torch.rand((10, 24, 3, 224, 224))  # 10 videos, 24 frames each, 3 channels, 224x224\n",
    "\n",
    "dataset = VideoDataset(video_data, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleViT, self).__init__()\n",
    "        # Pretrained ViT model\n",
    "        self.vit = models.vit_b_16(pretrained=True)\n",
    "        self.linear = nn.Linear(1000, 3 * 224 * 224)  # Adjust according to output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, F, C, H, W = x.shape  # B: batch size, F: num_frames\n",
    "        x = x.view(B * F, C, H, W)  # Flatten frames into batch\n",
    "        x = self.vit(x)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(B, F, C, H, W)  # Reshape to original frame shape\n",
    "        return x\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate model and move it to the GPU\n",
    "model = SimpleViT().to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Adjust number of epochs as needed\n",
    "    for videos in dataloader:\n",
    "        # Move videos to GPU\n",
    "        videos = videos.to(device)\n",
    "        \n",
    "        # Example of masked input (set parts of the video to zero)\n",
    "        masked_videos = videos.clone()\n",
    "        mask = torch.rand(masked_videos.shape).to(device) < 0.5  # Example mask (50% of data)\n",
    "        masked_videos[mask] = 0\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(masked_videos)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, videos)  # Compare reconstruction with original\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# After training, you can use the model to reconstruct missing values\n",
    "# Example of reconstructing missing values in a new video\n",
    "new_video = torch.rand((1, 24, 3, 224, 224)).to(device)\n",
    "masked_new_video = new_video.clone()\n",
    "mask = torch.rand(masked_new_video.shape).to(device) < 0.5\n",
    "masked_new_video[mask] = 0\n",
    "\n",
    "# Reconstruct the video using the model\n",
    "reconstructed_video = model(masked_new_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad16a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_output(output_image):\n",
    "    # output_image 形状：[batch_size, time_frames, channels, height, width]\n",
    "    output_image = output_image.squeeze(0).detach()  # 移除 batch 维度，并断开与计算图的连接\n",
    "    output_image = output_image.cpu().numpy()  # 将张量转换为 NumPy 数组\n",
    "    \n",
    "    time_frames = output_image.shape[0]  # 时间帧数\n",
    "    channels = output_image.shape[1]     # 通道数\n",
    "    height = output_image.shape[2]\n",
    "    width = output_image.shape[3]\n",
    "    \n",
    "    fig, axes = plt.subplots(channels, 4, figsize=(16, channels * 4))\n",
    "\n",
    "    for channel in range(channels):\n",
    "        for frame in range(4):  # 只显示前4帧影像\n",
    "            ax = axes[channel, frame] if channels > 1 else axes[frame]\n",
    "            ax.imshow(output_image[frame, channel, :, :], cmap='viridis')\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f'Channel {channel+1}, Frame {frame+1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 假设 `output_image` 是模型输出\n",
    "# output_image = model(data_sample)\n",
    "\n",
    "# 可视化结果\n",
    "visualize_output(output_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81413d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT",
   "language": "python",
   "name": "vit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
